# DeepD: Exploring linguistic pattern learned by Deep Tech for Depression Detection

Depression detection from text can serve as an valuable tool to early detect and intervene individuals suffering from depression. With popularity of social media, there is abundant text data generated by individual which can be leveraged to detect depression. This paper addresses two aspect of depression detection from social media text - using Bidirectional Encoder Representations from Transformers (BERT) language model for depression classification to achieve state of the art performance and to explore interpretability of BERT learning by leveraging \textsc{BERT} attention scores and benchmarking it against key lingustic indicators for depression, as reported by researchers analyzing vocabulary and semantic based features. Such benchmarking will help to encourage a sense of "trust" among researchers and medical/healthcare practitioners to help build tools around these models to be used for depression detection.
